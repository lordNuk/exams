# UNIT â€“ I 
- Data Warehouse: 
    - A Brief History, Characteristics, 
    - Architecture for a Data Warehouse. 
- Data Mining: 
    - Introduction, Motivation, Importance, 
    - Knowledge Discovery Process, 
    - Data Mining Functionalities, Interesting Patterns, 
    - Classification of Data Mining Systems, Major issues; 
- Data Pre-processing: 
    - Overview, Data Cleaning, Data Integration, 
    - Data Reduction, Data Transformation and Data Discretization, 
    - Outliers. 

## Data Warehouse
- it's a RDB used for the working of TPS(transaction procession system).
- it can be loosely described as any centralized data repository which can be queried for business benefits. 
- it's a db that contains info specific to dicision making requests. 
- it's a group of decision support technologies to enable knowledge worker to make good decisions.
- so data warehouse support archt. and tool for business exec.s to systematically organize, understand and use info to make strategic decisions.

- data warehouse env. contains: 
    1. extraction, transportation and loading (ETL) solution
    2. online analytical procession (OLAP) engine
    3. other apps that handle gathering and delivering info to business users.

- What is a Data Warehouse?
    - A Data Warehouse (DW) is a relational database that is designed for query and analysis rather than transaction processing. It includes historical data derived from transaction data from single and multiple sources. 
    - A Data Warehouse provides integrated, enterprise-wide, historical data and focuses on providing support for decision-makers for data modeling and analysis.
    - A Data Warehouse is a group of data specific to the entire organization, not only to a particular group of users.
    - It is not used for daily operations and transaction processing but used for making decisions.

- A Data Warehouse can be viewed as a data system with the following attributes:
    1. It is a database designed for investigative tasks, using data from various applications.
    2. It supports a relatively small number of clients with relatively long interactions.
    3. It includes current and historical data to provide a historical perspective of information.
    4. Its usage is read-intensive.
    5. It contains a few large tables.

*"Data Warehouse is a subject-oriented, integrated, and time-variant store of information in support of management's decisions."*

**Characteristics of Data Warehouse**

1. Subject-Oriented
    - A data warehouse target on the modeling and analysis of data for decision-makers. 
    - Therefore, data warehouses typically provide a concise and straightforward view around a particular subject, such as customer, product, or sales, instead of the global organization's ongoing operations.
    - This is done by excluding data that are not useful concerning the subject and including all data needed by the users to understand the subject.
2. Integrated
    - A data warehouse integrates various heterogeneous data sources like RDBMS, flat files, and online transaction records.
    - It requires performing data cleaning and integration during data warehousing to ensure consistency in naming conventions, attributes types, etc., among different data sources.
3. Time-Variant
    - Historical information is kept in a data warehouse. For example, one can retrieve files from 3 months, 6 months, 12 months, or even previous data from a data warehouse. These variations with a transactions system, where often only the most current file is kept.
4. Non-Volatile
    - The data warehouse is a physically separate data storage, which is transformed from the source operational RDBMS.
    - The operational updates of data do not occur in the data warehouse, i.e., update, insert, and delete operations are not performed. 
    - It usually requires only two procedures in data accessing: Initial loading of data and access to data. 
    - Therefore, the DW does not require transaction processing, recovery, and concurrency capabilities, which allows for substantial speedup of data retrieval.
    - Non-Volatile defines that once entered into the warehouse, and data should not change.

**History of Data Warehouse**
- The idea of data warehousing came to the late 1980's when IBM researchers Barry Devlin and Paul Murphy established the "Business Data Warehouse."
- In essence, the data warehousing idea was planned to support an architectural model for the flow of information from the operational system to decisional support environments. The concept attempt to address the various problems associated with the flow, mainly the high costs associated with it.
- In the absence of data warehousing architecture, a vast amount of space was required to support multiple decision support environments. In large corporations, it was ordinary for various decision support environments to operate independently.

**Data warehouse**
- Data warehouses generalize and consolidate data in multidimensional space.
- The construction of data warehouses involves data cleaning, data integration, and data transformation, and can be viewed as an important preprocessing step for data mining. 
- DW also provide OLAP tools for the interactive analysis of multidimensional data of varied granularities, which facilitates effective data generalization and data mining. 
- Many other data mining functions, such as association, classification, prediction, and clustering, can be integrated with OLAP operations to enhance interactive mining of knowledge at multiple levels of abstraction. 
- Hence, the data warehouse has become an increasingly important *platform* for data analysis and OLAP and will provide an effective platform for data mining.
- Therefore, data warehousing and OLAP form an essential step in the knowledge discovery process.

**Basic concepts**
- What is Data Warehouse?
    - it's a platform that provides architecture and tools for business executives to systematically organize, understand, and use their data to make good decisions.
    - have been defined in many ways, making it difficut to formulate a rigorous definition.
    - loosely, it can be defined as a data repo maintained seperately from the organizational databases that allows for integration of variety of application systems and support info processing by a solid platform of consolidate historic data for analysis.
    - acc to william h. inmon: *"a dw is a subject oriented, integrated, time-variant and non-volatile collection of data in support of management's decision making process"*.
    - key characteristics:
    1. Subject oriented: provides simple and concise view of particular subject issues.
    2. integrated: constructed by integrating multiple heterogeneous sources like rdbms, flat files, online transaction records, etc. Data cleaning and integration techs are applied to ensure consistency.
    3. time variant: data are stored to provide info from an historic prespective. Every key structure in dw contains, either implicitly or explicitly a time element.
    4. Nonvolatile: dw is a physically seperate store. it doesn't require transaction processing, recovery, and concurrency control mechanisms. It ususally req. only 2 operations initial load and access of data.

    - in sum, dw is a symentically consistent data store that serves as the physical implementation of a decision support data model.
    - it's also viewed as an architecture constructed by integrating data from multiple heterogeneous sources to support structured and/or ad hoc queries, analytical reporting, and decision making.
    - dw construction requires data cleaning, data integration and data consolidation.

## Data warehouse Architecture
- three tiered architecture
    1. bottom tier: dw server
    2. middle tier: olap server
    3. top tier: front-end tools
1. Bottom tier:
    - its a warehouse database server that is almost always a rdbs.
    - backend tools and utilities are used to feed data into this tier from operational dbs or other ext. sources.
    - these tools perform: data extraction, cleaning, transformation, and also load and refresh functions to update the dw.
    - data is extracted using api's known as gateways, which is supported by the underlying dbms and allows client programs to generate sql code to be executed at a server. for ex: odbc oledb jdbc.
    - it also contains a metadata repo, that stores info about dw and it's contents.
2. Middle tier:
    - it's and olap server that is typically implemented using either a relational OLAP model or a multi-demensional OLAP model.
3. Top tier:
    - its a front end client layer, containing query and reporting tools, analysis tools, and/or data mining tools.

- from architecture pov there are 3 data warehouse models:
    1. Enterprise warehouse
        - collects all info. about subjects spanning the entire org.
        - provides corporate-wide data integration, usually from 1 or more op systems or ext info provider and is cross-functional is scope.
        - constains detailed as well as summarised data from few gigs to terabytes and beyond.
        - may be implemented on traditional mainframes, superservers or paralled architecture platforms.
        - requires extensive business modeling and may take years to design and buid.
    2. Data mart
        - contains a subset of corporate wise data that is of use to a specifig group of users, scope is confined to specific selected subjects.
        - data contained tends to summarized.
        - implemented on low-cost departmental servers that are Unix/linux or window based.
        - implementational cycle is likely in weeks.
        - may involve complex integration in the long run if its design and planning were not enterprise wide.
        - 2 types: dependent and independent dm's based on source:
            - independent marts are sourced from data captured from one or more op systems or ext. info providers, or from data generated locally within a particualr department or geographic area.
            - dependent data marts are sourced directly from enterprise wide data warehouses.
    3. Virtual warehouse
        - set of views over op dbs
        - for efficient query processing, only some of the possible summary views may be materialize.
        - is easy to build but requires excess capacity on op db servers.

## Data Mining (KDD)
- concepts and techniques for discovering interesting patterns from data in various applications.
- are we living in the era of "information age" or the "data age"?
- explosive growth of available data as a result of computerization of society and fast development in the data collection and storage tools.
- so a data age then.
- powerful and versatile tools are badly needed to uncover/dig valuable info. from the tremendous amounts of data. leading to the birth of data mining.
- data mining can be viewed as a result of natural evolution of IT.
- data mining is a tool that can turn "data tombs" into "golden nuggets" of knowledge.

### Knowledge discover process
- iterative sequence of steps:
    1. Data cleaning: remove noise and inconsistent data
    2. Data integration: combining multiple data sources
    3. Data selection: relevant data are retrieved
    4. Data transformation: consolidation into forms appropriate fro data mining (summary/aggregation operations)
    5. Data mining: intelligent methods are applied to extract data patterns
    6. Pattern evaluation: identify truly interesting patterns representing knowledge based on interestingness measures.
    7. Knowledge presentation: visualization/representation techniques
- steps 1 - 4 comes under data pre-processing.
- instead of simply the 5th step in KDD, data mining usually reffers to the entire knowledge discovery process.
- so "data mining is the process of discovering interesting patterns and knowledge from large amounts of data."
- The data sources can include db, dw, web, other info reps, or data that are stereamed into the sys dynamically.


### Data Mining Functionalities
- data mining functionalities include:
    - characterizaiton and discrimination
    - classification and regression
    - mining of frequent patterns
    - association and correlation
    - clustering analysis
    - outer analysis
- these functionalities are used to specify the kinds of patterns to be found in data mining tasks.
- such tasks can be characterized into 2 categories:
    1. descriptive:
        - these dm tasks characterize properties of the data in data set.
    2. predictive:
        - perform induciton on the current data to make predictions.

- functionalities and the patterns they discover:
1. Class/Concept Description: Characterization and Discrimination
- data entries can be associated with classes or concepts.
- useful to describe individual classes/concepts in summarized, concise and yet precise terms.
- such descriptions of class/concept are called c/c disc.
- these descriptions can be derived from:
    - data characterization
    - data discrimination
    - both
- data characterization: 
    - summarization of genearl characteristics or features of target class of data.
    - corresponding data are usually collected by query.
    - several methods:
        - summaries based on statistical measures
        - data cube-based olap rollup operation
        - attribute-oriented induction technique
        - output formats:
        - pie charts, bar charts
        - curves, 
        - multi dementional data cubes,
        - multi dimentional tables,
        - generalized relations/ characteristic rules
- data discrimination:
    - comparison of general features of target class data obj.s against one or more multiple contrasting classes.
    - classes defined by users & data collected by queries.
    - methods: same as data characterization
    - output formats:
        - o/p format for data char. + comparative measures
        - rules format are reffered to as discriminant rules.
2. Mining frequent patterns, Associations, and colrelations:
    - frequent patterns, patterns that occur frequently..
    - many kinds including:
    - frequent itemsets, frequent subsequences (sequential patterns), frequent sub structures.
    - frequent itemsets: set of items that often appear together in transactional data set. eg. milk, bread, butter, etc.
    - frequent subsequences: set of items that often apper one after other in sequencial pattern.
    - frequent substurctures: reffer to different structural froms (like graphs, trees, lattices) that may be combined with itemsets or subsequences. If a structure occur frequently that is called a frequent structured pattern.
    - mining frequent ptterns leads to discovery of interesting associations and correlations within data.
    - frequent itemset mingin is a fundamental form of frequent pattern mining.
3. Classification and Regression form Predicitve analysis:
    - classification:
        - is the process of finding a model that describes and distinguishes data classes or concepts.
        - model is derived based on analysis of a set of training data.
        - model is used to predict the class label for which class label is unknown.
        - how is this derived model presented?
        - various forms like classificaiton rules, decision trees, mathematical formulaes, neutral networks.
    - regerssion: 
        - models continuous-valued function. ie, it is used to predict mission or unavailable numerical data values rather than (discrete) class labels.
        - prediction reffers to both numeric and class label prediction.
        - regression analysis is a statictical methodology that is most often used for numeric prediction, although other methods exist as well.
        - also includes identificaiton of distribution trends based on the available data.
        - class. & reg. may need to be preceded by relevance analysis, which attempts ot idenify attributes that are significantly relevant to the classification and regression process.
4. Cluster Analysis:
    - unlike class. & reg., clustering analyzes data objects without consulting class labels.
    - can be used to generate class labels for a group of data.
    - objects are clustered based on the principle of *maximizing the itraclass similarity & minimizing the inter class similarity*.
    - each cluster can be viewed as class of objects, from which rules can be derived.
    - clustering can also ficilitate **taxonomy formation**, ie, organiztion of observaitons into a heirarchy of classes that group similar events together.
5. Outlier Analysis:
    - data set may contain objs that don't comply with the general behaviour/model of data, called outliers.
    - many data mining methods discard outliers.
    - but in some applications the rare events may be more interesting, and analysis of such data is reffered to as outlier analysis or anomaly mining.
    - detected using:
        - statistical tests
        - distance measures (clusters)
        - density-based methods for local region.

### Interesting Patterns
- among the potentially thousands or even millions of patterns, or rules generated using data mining, only a small fraction would actually be of interest to a given user.
- what makes the patterns interesting?
    - those patterns that are:
    1. easily understood by humans
    2. valid on new test data with some degree of certainty,
    3. potentially usefull, &
    4. novel.
    - a pattern is also interesting if it validates some hypothesis that the user sought to confirm
    - interesting patterns represent knowledge.
- **objective measures of pattern interestingness**
    - based on discovered patterns & the stats underlying them.
    - objective measures of association rules of the form x=>y are:
    1. rule support: represent the percentage of transactions from the transaction db that the given rule satisfies. = P(x U y)
    2. confidence: assesses the degree of certainty of the detected association. = P(x|y)
    - generally interestingness measure is associated with a *conficence threshold*, that may be user controlled. Patterns that don't satisfy this are considered uninteresting.
    - obj. measures for classification rules include:
    1. accuracy: tells the %age of data correctly classified by a rule.
    2. coverage: tells us the percentage of data to which a rule applies, simillar to support for association.
    - regarding understandability we may use simple obj. measures that assess the complexity or length in bits of the patterns mined.
- although obj. measures help identify interesting patterns, they are often insufficient unless combined with subjective measures that reflects a perticular user's need and interests.
- **subjective measures**
    - based on user beliefs in data.
    - they find patterns interesting if the patterns are i) unexpected or ii) offer strategic info. on which the user can act (actionable).
    - also expected patterns can be interesting if they confirm a hypothesis that the user wishes to validate.
- can data mining generate all the interesting patterns?
    - depends on the completeness of the data mining algo.
    - it's unrealistic and inefficient to gen. all int. patterns.
    - constraints are provided by users and interesting measures are used to focus the search.
    - but in case of association rule mining use of const. and int. measures can ensure the completeness of mining.
- can data mining system generate only interesting patterns?
    - its and optimization problem in data mining.
    - its highly desirable for dms to gen only interesting patterns, as it would be efficiend for the users.
    - progress have been made in this direction, still it remains a challegining issue in data mining.
    
### Major issues in Data Mining
- these issues are partitioned into five groups:
    1. mining methodology
    2. user interaction
    3. efficiency & scalability
    4. diversity of data types
    5. data mining and society

1. Mining methodology
- researchers are vigorously developing new dm methodologies.
- various aspects of dm methodology include:
    1. mining various and new kinds of knowledge
    2. Mining knowledge in multidementional space
    3. data mining - an interdisciplinary effort
    4. boosting the power of mining in a netowrked env.
    5. handling uncertainty, noise or incompleteness of data.
    6. pattern evaluation and pattern- or constraint-guided mining.

2. User Interaction
- how to interact with dms
    - build flexible user interfaces and exploratory mining env.
    - give user the power...
- how to incorporate user's background knowledge in mining,
- ad hoc data mining and dm query languages:
    - query languages is imp. for flexible searching as they allow ad hoc queries.
- how to visualize and comprehend data mining results.

3. Efficiency and Scalability
- always considered when comparing dm algo.s
    1. efficency and scalability of dm algo.s
    2. parallel, distributed and incremental algo.s
    3. cloud computing and cluster computing

4. Diversity of db types
- wide diversity of db types brings about challenges to dm.
    1. handlign complex types of data
    2. mining dynamic, networked, and global data repos

5. Data Mining and society
    1. social impacts of dm: 
    2. privacy-preserving dm
    3. invisible dm


## Data Pre-processing: 
    
### Overview
- improve quality of data and it turn data mining results.
- inc. efficiency and ease of dm process.
1. **Data Quality Criteria:** 
    - Data quality is determined by its ability to meet the requirements of its intended use. 
    - Various factors contribute to data quality, including accuracy, completeness, consistency, timeliness, believability, and interpretability.

2. **Reasons for Inaccurate Data:**
    - Faulty data collection instruments.
    - Human or computer errors during data entry.
    - Deliberate submission of incorrect values (disguised missing data).
    - Errors in data transmission.
    - Technology limitations.

3. **Incomplete Data Causes:**
    - Attributes may not always be available.
    - Data may be omitted if deemed unimportant during entry.
    - Relevant data may not be recorded due to misunderstandings or equipment malfunctions.
    - Deletion of data inconsistent with others.
    - Overlooking recording of data history or modifications.

4. **User Perspective on Data Quality:** 
    - Data quality depends on the intended use. 
    - Different users may assess the quality differently based on their specific needs and expectations.

5. **Timeliness Impact on Data Quality:** 
    - Delayed submission of sales records and late corrections impact the completeness of the data, affecting its overall quality.

6. **Believability and Interpretability:** 
    - **Believability:** Reflects the trustworthiness of the data. Users may lose trust in data due to past errors, even if corrected.
    - **Interpretability:** Reflects how easily users can understand the data. Complex codes or conventions may hinder interpretability.

- In summary, the quality of data is a multifaceted aspect influenced by various factors, and understanding these factors is essential for effective data analysis and decision-making.

- real-world data tend to be dirty, incomplete, and inconsistent. Data pre-processing techniques can improve data quality, thereby helping to improve the accuracy and efficiency of the subsequent mining process. 
- Data preprocessing is an important step in the knowledge discovery process, because quality decisions must be based on quality data. 
Detecting data anomalies, rectifying them early, and reducing the data to be analyzed can lead to huge payoffs for decision making

### Data Cleaning
- remove noise and inconsistencies in data
- Real-world data tends to be incomplete, noisy, and inconsistent.
- Data cleaning routines aim to fill in missing values, smooth out noise, and correct inconsistencies.

1. **Handling Missing Values:**
- Methods:
    1. Ignore the tuple: Not effective unless multiple attributes have missing values.
    2. Fill in manually: Time-consuming and may not be feasible for large datasets.
    3. Use a global constant: Replace missing values with a constant, but can introduce biases.
    4. Use central tendency measures: Replace with mean or median based on data distribution.
    5. Use class-specific mean or median: Replace with mean or median for the same class.
    6. Use the most probable value: Determined using regression, Bayesian tools, or decision tree induction. Popular despite potential bias.
    - Methods 3 to 6 introduce bias as the filled-in value may not be correct.
    - Method 6, despite bias, is popular as it utilizes the most information to predict missing values, preserving relationships between attributes.
- Handling Missing Values - Important Considerations:
    - Missing values may not imply errors; candidates may leave fields blank if not applicable.
    - Forms should allow respondents to specify values like "not applicable."
    - Software routines can uncover null values such as "don't know" or "none."
    - Each attribute should have rules regarding null conditions to guide handling or transformation.
- Preventing Missing Values:
   - Good database and data entry design can help minimize missing values and errors.
   - Ideally, rules should specify whether nulls are allowed and how they should be handled or transformed.

2. **Noise:**
- Noise is random error or variance in a measured variable.
- Identification of Noise: 
    - Statistical description techniques (e.g., boxplots, scatter plots) and data visualization methods help identify outliers that may represent noise.

- Data Smoothing Techniques: binning, regression, outiler analysis.
- Binning:
    - Sorted data values are distributed into bins, and smoothing is performed by consulting the neighborhood of values.
    - Smoothing methods include bin means, bin medians, and bin boundaries.
    - Binning is also used as a discretization technique.
- Regression:
    - Conforms data values to a function, such as linear regression or multiple linear regression.
    - Linear regression finds the best line to fit two attributes for prediction.
- Outlier Analysis:
    - Outliers may be detected through clustering, where values falling outside clusters are considered outliers.

- Data Discretization and Reduction:
   - Many data smoothing methods are used for data discretization and reduction.
   - Binning reduces the number of distinct values per attribute, aiding logic-based data mining methods.
   - Concept hierarchies map real values into categories, reducing the number of data values.

- Classification and Data Smoothing:
   - Some classification methods (e.g., neural networks) inherently have built-in data smoothing mechanisms.

- In summary, noise in data, characterized by random errors, can be addressed through various data smoothing techniques, including binning, regression, and outlier analysis. These methods also play a role in data discretization and reduction, enhancing the effectiveness of data mining processes.

3. **Data cleaning as a process:**
- Data cleaning is a process involving discrepancy detection and data transformation.
- steps include:

- Discrepancy Detection:
    - Discrepancies arise from poorly designed forms, human errors, deliberate errors, data decay, inconsistent representations, and use of codes.
    - Metadata (data about data) is essential for understanding properties and trends.
    - Statistical data descriptions and domain knowledge aid in discrepancy detection.
    - Unique rules, consecutive rules, and null rules are examined to identify discrepancies.

- Data Transformation:
    - Commercial tools (data scrubbing, data auditing, ETL) assist in discrepancy detection.
    - Data migration tools and ETL tools facilitate data transformation through GUI or custom scripts.
    - Discrepancy detection and data transformation are iterative processes, prone to errors and time-consuming.

- Iterative Process Challenges:
    - Some discrepancies may only be detected after others are fixed.
    - Batch transformations may introduce new errors.
    - Lack of interactivity in the traditional process leads to lengthy iterations.

- Increased Interactivity Approaches:
    - Tools like Potter's Wheel offer increased interactivity by integrating discrepancy detection and transformation.
    - Potter's Wheel provides a spreadsheet-like interface for composing and debugging transformations, with immediate feedback and undo options.
    - Declarative languages for data transformation operators aim to enhance interactivity and efficiency.

- Updating Metadata:
    - As knowledge about the data grows, metadata should be updated to reflect changes.
    - Updated metadata facilitates faster data cleaning in future versions of the data store.

In summary, data cleaning involves detecting and correcting discrepancies, often with the assistance of commercial tools. The iterative process of discrepancy detection and data transformation is challenging, leading to the development of more interactive tools and approaches. Keeping metadata updated is crucial for efficient data cleaning in subsequent versions.

In summary, data cleaning is crucial to address real-world data challenges, especially concerning missing values, and involves various methods with considerations for potential biases and data integrity.

### Data Integration 
- merge data from * sources into a coherent data source (dw).
- Careful integration reduces redundancies and inconsistencies, enhancing accuracy and speed in the data mining process.
- Semantic heterogeneity and data structure pose challenges in integrating data.

1. Entity Identification Problem:
  - Data analysis tasks often involve data integration for coherent data warehousing.
  - Entity identification problem: Matching equivalent real-world entities from multiple sources.
  - Metadata (name, meaning, data type, range of values, null rules) aids in avoiding errors in schema integration.
  - Matching attributes requires consideration of data structure to ensure functional dependencies and constraints match between source and target systems.
2. Redundancy in Data Integration:
  - Redundancy occurs when an attribute can be derived from another or due to inconsistencies in attribute or dimension naming.
  - Correlation Analysis for Redundancy Detection:
  - Correlation analysis helps detect redundancies in data integration.
  - Nominal Data: Ï‡2 (chi-square) test measures correlation between two attributes, testing the hypothesis of independence.
  - Numeric Data: Correlation coefficient measures correlation, ranging from -1 to +1.
    - Positive correlation (r > 0) indicates attributes increase together.
    - Zero correlation (r = 0) means no correlation.
    - Negative correlation (r < 0) indicates attributes vary inversely.
  - Ï‡2 Correlation Test for Nominal Data:
    - Measures the strength of correlation between two nominal attributes (A and B).
    - Utilizes a contingency table with observed and expected frequencies.
    - Ï‡2 statistic tests the hypothesis of independence.

3. Tuple Duplication:
  - Duplication detection is crucial at the tuple level, where identical tuples for a unique data entry case need to be identified.
  - Denormalized tables, often used for performance reasons, can lead to data redundancy.
  - Inconsistencies may arise in duplicates, especially when denormalization is used without proper synchronization.

2. Data Value Conflict Detection:
  - Data integration involves detecting and resolving conflicts in attribute values from different sources.
  - Conflicts may arise due to differences in representation, scaling, encoding, or other factors.
  - Examples include variations in units (e.g., metric vs. imperial), currencies, services, taxes, curricula, grading schemes, and abstraction levels.
  - Resolution of Data Value Conflicts:
    - Challenges include establishing rules for transformation between different representations and resolving differences in abstraction levels.
    - Attributes may represent the same concept but at different abstraction levels across systems.

In summary, redundancy in data integration can be detected through correlation analysis. Different tests are used for nominal and numeric data, such as the Ï‡2 test and correlation coefficient. Covariance and variance provide additional insights, and it's crucial to understand that correlation doesn't imply causality.

### Data Reduction
- reduce data size, by aggregation, eleminating redundant features, clusterning, etc.
- Objective: Handle large datasets for efficient analysis and mining.
- Strategies: Dimensionality reduction, numerosity reduction, and data compression.
  
- **Dimensionality Reduction**
1. Wavelet Transforms (Section 3.4.2):
  - Linear signal processing technique.
  - Transform data into wavelet coefficients.
  - Allows for sparse data representation and noise removal.
  - Suitable for multidimensional data and various applications.
  
2. Principal Components Analysis (PCA; Section 3.4.3):
  - Reduces dimensions by finding orthogonal vectors.
  - Projects data onto a smaller space, preserving variance.
  - Reveals relationships and aids interpretation.
  - Effective for ordered and unordered attributes.

3. Attribute Subset Selection (Section 3.4.4):
  - Reduces data size by removing irrelevant or redundant attributes.
  - Avoids confusion and accelerates mining process.
  - Heuristic methods like stepwise selection and decision tree induction.

- **Numerosity Reduction**
4. Regression and Log-Linear Models (Section 3.4.5):
  - Parametric methods for estimating data using models.
  - Linear regression models data as a straight line.
  - Log-linear models estimate discrete multidimensional probability distributions.
  - Applicable to sparse and skewed data, but computational intensity may vary.

- **Data Compression**
5. Histograms (Section 3.4.6):
  - Use binning to approximate data distributions.
  - Effective for sparse, dense, skewed, and uniform data.
  - Multidimensional histograms capture dependencies.

6. Clustering (Section 3.4.7):
  - Partition data into clusters based on similarity.
  - Replaces actual data with cluster representations.
  - Effective for organized data but less so for smeared data.

7. Sampling (Section 3.4.8):
  - Represents large datasets with smaller random samples.
  - Types include simple random sample, cluster sample, and stratified sample.
  - Cost is proportional to sample size, making it potentially more efficient.

8. Data Cube Aggregation (Section 3.4.9):
  - Aggregates data in a data cube for multidimensional analysis.
  - Allows summarization at different abstraction levels.
  - Fast access to precomputed, summarized data for analytical processing and mining.

### Data Transformation & Data Discretization 
- for eg: normalization where data are scaled to fall within a smaller range, improves accuracy and efficiency of dm algorithms involving distance measures.
1. Data Transformation Strategies Overview:
  - Smoothing:
    - Purpose: Removes noise from data.
    - Techniques: Binning, regression, clustering.
  - Attribute Construction:
    - Purpose: Introduces new attributes to enhance the mining process.
  - Aggregation:
    - Purpose: Applies summary operations, useful for constructing data cubes.
  - Normalization:
    - Purpose: Scales attribute data for a smaller range.
  - Discretization:
    - Purpose: Replaces numeric attribute values for more efficient mining.
    - Types: Supervised (uses class information), unsupervised (doesn't use class information).
  - Concept Hierarchy Generation:
    - Purpose: Creates hierarchical structures for both numeric and nominal data.
    - Importance: Facilitates mining at different abstraction levels.

2. Normalization Techniques (Section 3.5.2):
  - Purpose:
    - Equalizes attribute weight for certain algorithms (e.g., neural networks).
  - Methods:
    - Min-max Normalization:
      - Linear transformation to map values to a specified range.

3. Discretization Techniques (Sections 3.5.3 - 3.5.5):
  - Binning (Section 3.5.3):
    - Top-down splitting technique.
    - Unsupervised method sensitive to user-specified bin count.
  - Histogram Analysis (Section 3.5.4):
    - Uses histograms for unsupervised concept hierarchy generation.
    - Partitioning rules: Equal-width and equal-frequency.
  - Cluster, Decision Tree, and Correlation Analyses (Section 3.5.5):
    - Cluster Analysis:
      - Clustering algorithm applied for numeric attribute discretization.
      - Considers distribution and closeness of data points.
    - Decision Tree Analysis:
      - Supervised method using class label information for discretization.
      - Entropy measurement used for split-point determination.
    - Correlation Analysis:
      - ChiMerge method for supervised discretization based on class information.

4. Concept Hierarchy Generation for Nominal Data (Section 3.5.6):
  - Nominal Attributes:
    - Finite distinct values with no ordering.
  - Methods:
    - User/Expert-defined Partial Ordering:
      - Explicit specification of a hierarchy at the schema level.
    - Explicit Data Grouping:
      - Manual definition of a portion of a concept hierarchy.
    - Automatic Generation based on Attribute Values:
      - Heuristic rule considers the number of distinct values per attribute.
    - Handling Partially Specified Hierarchies:
       - Embedding data semantics to complete hierarchies based on user specifications.
       - Example (3.8) illustrating hierarchy generation using prespecified semantic connections.

5. Conclusion:
  - Data Transformation Significance:
    - Simplifies data for more understandable patterns.
    - Facilitates mining at various abstraction levels.
  - Normalization and Discretization:
    - Forms of data reduction for efficiency.
  - Concept Hierarchies:
    - Vital for understanding patterns in data mining applications.

### Outlier Analysis:
- There exist data objects that do not comply with the general behavior or model of the data.
- Such data objects, which are grossly different from or inconsistent with the remaining set of data, are called outliers.
- Many data mining algorithms try to minimize the influence of outliers or eliminate them all together. This, however, could result in the loss of important hidden information because one personâ€™s noise could be another personâ€™s signal. In other words, the outliers may be of particular interest, such as in the case of fraud detection, where outliers may indicate fraudulent activity. Thus, outlier detection and analysis is an interesting data mining task, referred to as outlier mining.
- It can be used in fraud detection, for example, by detecting unusual usage of credit cards or telecommunication services. In addition, it is useful in customized marketing for identifying the spending behavior of customers with extremely low or extremely high incomes, or in medical analysis for finding unusual responses to various medical treatments.

- Outlier mining can be described as follows: Given a set of n data points or objects and k, the
expected number of outliers, find the top k objects that are considerably dissimilar,
exceptional, or inconsistent with respect to the remaining data. The outlier mining problem
can be viewed as two subproblems:
  - Define what data can be considered as inconsistent in a given data set, and
  - Find an efficient method to mine the outliers so defined.




























