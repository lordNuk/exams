# UNIT – II: Data Mining Techniques

## Cluster:
- clustering means groming groups.
- earliest data mining technique.
- unsupervised learning. (we don't know the class labels)
- algorithm searches for groups or clusters of data elements that are similar to one another.
- eg: k-means clustering
  - let there be a data of 4 students subject and marks.
  - me plot them in graph and make clusters of the points closer.
  - for any new data item, we add it to one clusters based of different *distance measuers*.
    - using centroid.
    - euclidian distance: (A, B) = sqrt(summations(i(1, n), (xi - yi)^2))

- The process of grouping a set of physical or abstract objects into classes of similar objects is called clustering.
- A cluster is a collection of data objects that are similar to one another within the same cluster and are dissimilar to the objects in other clusters.
- A cluster of data objects can be treated collectively as one group and so may be considered as a form of data compression.
- Cluster analysis tools based on k-means, k-medoids, and several methods have also been built into many statistical analysis software packages or systems, such as S-Plus, SPSS, and SAS.
- Clustering is also called data segmentation in some applications because clustering partitions large data sets into groups according to their similarity

### Advantages of clustering:
1. not affected by addition of new obj.
2. no need to worry about signs.
3. simple computation process(basic maths)

### Application of clustering:
Cluster analysis has been widely used in numerous applications, including market research, pattern recognition, data analysis, and image processing.

1. medical:
  - establish taxonomy of disease, cure and symptoms.
2. www:
  - recognize social network communities
3. siesmology:
  - for epicenter of earthquake
4. business marketing:
  - find the targeted customers.
  - In business, clustering can help marketers discover distinct groups in their customer bases and characterize customer groups based on purchasing patterns.
5. biology:
  - In biology, it can be used to derive plant and animal taxonomies, categorize genes with similar functionality, and gain insight into structures inherent in populations.
6. outlier detection:
  - include the detection of credit card fraud and the monitoring of criminal activities in electronic commerce.

- Clustering may also help in the identification of areas of similar land use in an earth observation database and in the identification of groups of houses in a city according to house type, value,and geographic location, as well as the identification of groups of automobile insurance policy holders with a high average claim cost.

### Requirements Of Clustering In Data Mining:
1. Scalability:
- Many clustering algorithms work well on small data sets containing fewer than several hundred data objects; however, a large database may contain millions of objects. Clustering on a sample of a given large data set may lead to biased results.
- Highly scalable clustering algorithms are needed.

2. Ability to deal with different types of attributes:
- Many algorithms are designed to cluster interval-based (numerical) data.
- However, applications may require clustering other types of data, such as binary, categorical (nominal), and ordinal data, or mixtures of these data types.

3. Discovery of clusters with arbitrary shape:
- Many clustering algorithms determine clusters based on Euclidean or Manhattan distance measures. 
- Algorithms based on such distance measures tend to find spherical clusters with similar size and density.
- However, a cluster could be of any shape. It is important to develop algorithms that can detect clusters of arbitrary shape.

4. Minimal requirements for domain knowledge to determine input parameters:
- Many clustering algorithms require users to input certain parameters in cluster analysis (such as the number of desired clusters). The clustering results can be quite sensitive to input parameters.
- Parameters are often difficult to determine, especially for data sets containing high-dimensional objects. This not only burdens users, but it also makes the quality of clustering difficult to control.

5. Ability to deal with noisy data:
- Most real-world databases contain outliers or missing, unknown, or erroneous data.
- Some clustering algorithms are sensitive to such data and may lead to clusters of poor quality.

6. Incremental clustering and insensitivity to the order of input records:
- Some clustering algorithms cannot incorporate newly inserted data (i.e., database updates) into existing clustering structures and, instead, must determine a new clustering from scratch. 
- Some clustering algorithms are sensitive to the order of input data. That is, given a set of data objects, such an algorithm may return dramatically different clusterings depending on the order of presentation of the input objects.
- It is important to develop incremental clustering algorithms and algorithms that are insensitive to the order of input.

7. High dimensionality:
- A database or a data warehouse can contain several dimensions or attributes.
- Many clustering algorithms are good at handling low-dimensional data,involving only two to three dimensions. 
- Human eyes are good at judging the quality of clustering for up to three dimensions. 
- Finding clusters of data objects in high dimensional space is challenging, especially considering that such data can be sparse and highly skewed.

8. Constraint-based clustering:
- Real-world applications may need to perform clustering under various kinds of constraints.
- Suppose that your job is to choose the locations for a given number of new automatic banking machines (ATMs) in a city. To decide upon this, you may cluster households while considering constraints such as the city’s rivers and highway networks, and the type and number of customers per cluster.
- A challenging task is to find groups of data with good clustering behavior that satisfy specified constraints.

9. Interpretability and usability:
- Users expect clustering results to be interpretable, comprehensible, and usable. That is, clustering may need to be tied to specific semantic interpretations and applications. 
- It is important to study how an application goal may influence the selection of clustering features and methods.

### Overview of clustering methods:
1. Partitioning Methods:
- A partitioning method constructs k partitions of the data, where each partition represents a cluster and k <= n. 
- That is, it classifies the data into k groups, which together satisfy the following requirements:
  - Each group must contain at least one object, and
  - Each object must belong to exactly one group.
- A partitioning method creates an initial partitioning. 
- It then uses an iterative relocation technique that attempts to improve the partitioning by moving objects from one group to another.
- The general criterion of a good partitioning is that objects in the same cluster are close or related to each other, whereas objects of different clusters are far apart or very different.

2. Hierarchical Methods:
- A hierarchical method creates a hierarchical decomposition of the given set of data objects. 
- A hierarchical method can be classified as being either agglomerative or divisive, based on how the hierarchical decomposition is formed.
- Theagglomerative approach, also called the bottom-up approach, starts with 
  - each object forming a separate group. 
  - It successively merges the objects or groups that are close to one another, until all of the groups are merged into one or until a termination condition holds.
- The divisive approach, also called the top-down approach, starts with 
  - all of the objects in the same cluster. 
  - In each successive iteration, a cluster is split up into smaller clusters, until eventually each object is in one cluster, or until a termination condition holds.
- Hierarchical methods suffer from the fact that once a step (merge or split) is done,it can never be undone. 
- This rigidity is useful in that it leads to smaller computation costs by not having to worry about a combinatorial number of different choices.
- There are two approaches to improving the quality of hierarchical clustering:
  - Perform careful analysis of object *linkages* at each hierarchical partitioning, such as in
Chameleon, or
  - Integrate hierarchical agglomeration and other approaches by first using a hierarchical agglomerative algorithm to group objects into microclusters, and then performingmacroclustering on the microclusters using another clustering method such as iterative relocation.

### Partitioning Methods:
1. The k-means Method
2. The k-mediods Method

1. **The k-means Method(centroid based technique)**:
- this algo takes the i/p parameter, k, and partition a set of n objects into k clusters so that the resulting intra cluster similarity is hihg but the intercluster similarity is low.
- cluster similarity is measured in regard to the mean value of the objects in a cluster, which can be viewed as the cluster's centroid or center of gravity.

- Input:
  - k: the number of clusters,
  - D: a data set containing n objects
- Output:
  - a set of k clusters.

- steps:
  1. randomly selects k of the objects, each of which intially represents a cluster mean or center.
  2. repeat:
  3.    (re)assign each object to the cluster to which it is the most similar, based on the distance between the object and the cluster mean.
  4.    compute new mean for each cluster.
  5. until no change;

2. **The k-medoids Method**:
- the k-means algorithm is sensitive to outliers because an object with an extremely large value may substantially distort the distribution of data. This effect is particularly exacerbated due to the use of the square-error function.
- Instead of taking the mean value of the objects in a cluster as a reference point, we can pick actual objects to represent the clusters, using one representative object per cluster. Each remaining object is clustered with the representative object to which it is the most similar.
- The partitioning method is then performed based on the principle of minimizing the sum of the dissimilarities between each object and its corresponding reference point. 
- That is, an absolute-error criterion is used, defined as
  - E is the sum of the absolute error for all objects in the data set
  - p is the point in space representing a given object in cluster Cj
  - oj is the representative object of Cj

- The initial representative objects are chosen arbitrarily. The iterative process of replacing representative objects by non representative objects continues as long as the quality of the resulting clustering is improved.
- This quality is estimated using a cost function that measures the average dissimilarity between an object and the representative object of its cluster.
- To determine whether a non representative object, oj random, is a good replacement for a current representative object, oj, the following four cases are examined for each of the non representative objects.
  - Case 1:
    - p currently belongs to representative object, oj. If oj is replaced by orandom as a representative object and p is closest to one of the other representative objects, oi,i≠j, then p is reassigned to oi.
  - Case 2:
    - p currently belongs to representative object, oj. 
    - If oj is replaced by orandom as a representative object and p is closest to orandom, then p is reassigned to orandom.
  - Case 3:
    - p currently belongs to representative object, oi, i≠j. 
    - If oj is replaced by orandom as a representative object and p is still closest to oi, then the assignment does not change.
  - Case 4:
    - p currently belongs to representative object, oi, i≠j. 
    - If oj is replaced by orandom as a representative object and p is closest to orandom, then p is reassigned to orandom.

- algo:
  - Input:
    - k: no. of clusters
    - D: data set of n objs
  - Output:
    - set of k clusters
  - Method:
    1. arbitrarily choose k objects in D as the initial representative objects or seeds;
    2. repeat:
    3.    assing each remaining obj to the cluseter with the nearest rep. obj.
    4.    randomly select a non rep. obj, orandom;
    5.    compute the total cost, S of swapping rep obj, oj, with orandom;
    6.    if S < 0 then swap oj with orandom to form the new set of k rep objects;
    7. until no change;

- the k-medoids method is more robust than k-means in the presence of noise and outliers because a medoid is less influenced by outliers or other extreme values than a mean. However its processin gis more costly than the k-means method.

### Hierarchical Clustering Methods:
1. Agglomerative Hierarchical Clustering
2. Divisive Hierarchical Clustering

- hierarchical clustering method works by grouping data objects into a tree of clusters.
- the quality of a pure hierarchical clustering method suffers from its inability to perform adjustment once a merge or split decision has been executed. That is, if a particular merge or split decision later turns out to have been a poor choice, the method cannot backtrack and correct it.

Hierarchical clustering methods can be further classified as either agglomerative or divisive, depending on whether the hierarchical decomposition is formed in a bottom-up or top-down fashion.

1. Agglomerative hierarchical clustering:
- This bottom-up strategy starts by placing each object in its own cluster and then merges these atomic clusters into larger and larger clusters, until all of the objects are in a single cluster or until certain termination conditions are satisfied.
- Most hierarchical clustering methods belong to this category. They differ only in their definition of intercluster similarity.

2. Divisive hierarchical clustering:
- This top-down strategy does the reverse of agglomerative hierarchical clustering by starting with all objects in one cluster.
- It sub divides the cluster into smaller and smaller pieces, until each object forms a cluster on its own or until it satisfies certain termination conditions, such as a desired number of clusters is obtained or the diameter of each cluster is within a certain threshold

---

## Decision Tree:

Decision trees are a popular machine learning technique used in the field of data mining for classification and regression tasks. They are a type of predictive model that maps features (attributes) of a dataset to its target variable (class label or numeric value). Decision trees organize these mappings into a hierarchical structure, resembling a flowchart, where each internal node represents a decision based on an attribute, each branch corresponds to the outcome of that decision, and each leaf node holds the predicted target variable.

**Key Components of Decision Trees:**

1. **Root Node:** The topmost node in the tree, representing the entire dataset.
2. **Internal Nodes:** Nodes that test a specific attribute, leading to different branches based on the attribute's values.
3. **Branches:** Paths leading from internal nodes to other nodes or leaves based on attribute values.
4. **Leaves:** Terminal nodes that hold the predicted target variable.

### Decision Tree Induction:

Decision tree induction is the process of creating a decision tree from a labeled training dataset. The goal is to create a tree that generalizes well to new, unseen data. 

The process involves recursively splitting the dataset based on attribute tests, selecting the best attributes at each step, and determining the optimal splitting criteria.

**Steps in Decision Tree Induction:**
1. **Root Node Creation:** 
  - Begin with a root node representing the entire dataset.

2. **Attribute Selection:** 
   - Choose the best attribute for splitting, typically based on measures like Information Gain, Gini Index, or Gain Ratio.
   - The selected attribute should best discriminate the tuples in the dataset according to their classes.

3. **Splitting Criteria:** 
   - Define criteria for splitting based on the chosen attribute.
   - Criteria vary based on the attribute type:
     - For continuous attributes: A threshold is chosen, and tuples are split into two groups based on whether their attribute values are above or below the threshold.
     - For discrete attributes: All possible values may lead to separate branches, and each branch represents a specific attribute value.

4. **Recursive Splitting:** 
   - Create child nodes for each branch, representing subsets of the dataset.
   - Recursively apply the decision tree induction process to each subset until termination conditions are met.

5. **Termination Conditions:** 
   - Decision tree induction stops when certain conditions are satisfied, such as:
     - All tuples in a subset belong to the same class.
     - A predefined depth of the tree is reached.
     - A minimum number of tuples in a node is reached.
     - No further attribute to split on.

6. **Leaf Node Assignment:** 
   - Once a termination condition is met, assign a class label or numeric value to the leaf node.
   - The assignment is typically based on the majority class in the subset for classification tasks or the average target variable for regression tasks.

7. **Recursive Process:**
   - The recursive nature of decision tree induction means that each internal node represents a decision based on an attribute, and each leaf node represents a final decision or prediction.

8. **Handling New Instances:**
   - When a new instance needs classification or regression prediction, it traverses the decision tree, following the path based on attribute tests until it reaches a leaf node, which provides the final prediction.

### Algo for decision tree induciton:
Generate a decision tree from the training tuples of data partition, D.

**Input:**
- Data partition, D: a set of training tuples and their associated class labels;
- attribute list: the set of candidate attributes;
- Attribute selection method: a procedure to determine the splitting criterion that “best” partitions the data tuples into individual classes. This criterion consists of a splitting attribute and, possibly, either a split-point or splitting subset.

**Output:** A decision tree.

**Method:**
1. create a node N ;
2. if tuples in D are all of the same class, C, then
3.    return N as a leaf node labeled with the class C;
4. if attribute list is empty then
5.    return N as a leaf node labeled with the majority class in D; // majority voting
6. apply Attribute selection method(D, attribute list) to find the “best” splitting criterion;
7. label node N with splitting criterion;
8. if splitting attribute is discrete-valued and multiway splits allowed then // not restricted to binary trees
(9) attribute list ← attribute list − splitting attribute; // remove splitting attribute
(10) for each outcome j of splitting criterion // partition the tuples and grow subtrees for each partition
(11)    let Dj be the set of data tuples in D satisfying outcome j; // a partition
(12)    if Dj is empty then
(13)      attach a leaf labeled with the majority class in D to node N ;
(14)    else attach the node returned by Generate decision tree(Dj , attribute list) to node N ;
    endfor
(15) return N ;

**Advantages of Decision Trees in Data Mining:**
1. **Interpretability:** Decision trees are easy to understand and interpret, making them suitable for conveying insights to non-experts.
2. **No Data Normalization Required:** Decision trees do not require normalization of data, and they can handle both numerical and categorical features.
3. **Handling Non-linearity:** Decision trees can model non-linear relationships between features and target variables effectively.
4. **Variable Importance:** They provide information about the importance of different features in the classification or regression task.
5. **Speed of Prediction:** Decision trees can quickly predict the target variable for new instances once the tree is constructed.

In summary, decision trees in data mining provide a transparent and powerful method for creating predictive models from data, enabling effective classification and regression tasks. The induction process involves recursively partitioning the data based on attribute tests, leading to a tree structure that captures the decision-making process.

### Attribute Selection Measures:
1. **Definition:**
   - Attribute selection measures are heuristics for selecting the best splitting criterion in decision tree induction.
   - They determine how to partition a given data partition (D) into individual classes.

2. **Objective:**
   - The goal is to find a splitting criterion that results in pure partitions, where all tuples in a partition belong to the same class.

3. **Role in Decision Tree Induction:**
   - Attribute selection measures rank attributes based on their ability to discriminate training tuples.
   - The attribute with the best score is chosen as the splitting attribute for a given node.

4. **Popular Measures:**
  1. **Information Gain:**
    - **Definition:** Information gain is based on Claude Shannon's information theory and is employed by the ID3 decision tree algorithm.
    - **Objective:** It measures the reduction in entropy (information content) after splitting on an attribute.
    - **Calculation:** Information gain is the difference between the original information requirement (based on class proportions) and the new requirement after partitioning on the attribute.
    - **Selection Criteria:** The attribute with the highest information gain is chosen as the splitting attribute for a given node.

  2. **Gain Ratio:**
    - **Definition:** Gain ratio is an extension of information gain, designed to address its bias towards attributes with many values.
    - **Objective:** It normalizes information gain by considering split information, which represents the potential information generated by splitting the training data set.
    - **Calculation:** Gain ratio is the ratio of information gain to split information.
    - **Selection Criteria:** The attribute with the maximum gain ratio is selected as the splitting attribute.

  3. **Gini Index:**
    - **Definition:** The Gini index is used by the CART decision tree algorithm and measures the impurity of a data partition.
    - **Objective:** It quantifies the likelihood of incorrect classification if a tuple from the partition is randomly assigned a class label.
    - **Calculation:** For each attribute, binary splits are considered for discrete-valued attributes, and possible split points are evaluated for continuous-valued attributes. The Gini index is the weighted sum of impurities of resulting partitions.
    - **Selection Criteria:** The attribute with the minimum Gini index is chosen as the splitting attribute.

5. **Selection Criteria:**
   - The attribute with the highest information gain, gain ratio, or the minimum Gini index is chosen as the splitting attribute.

6. **Biases and Considerations:**
   - Information gain is biased toward multivalued attributes, while gain ratio attempts to address this bias.
   - Gini index is biased toward multivalued attributes and has difficulty with a large number of classes.
   - Shallow trees may be preferred due to lower time complexity, but the choice depends on specific scenarios.

7. **Other Measures:**
    - Various other measures exist, such as those based on statistical tests, MDL principle, and multivariate splits.
    - The choice of the best measure depends on specific characteristics of the data and the desired tree structure.

8. **No Universally Superior Measure:**
    - Despite comparative studies, no single attribute selection measure has been found to be significantly superior to others.
    - Most measures yield satisfactory results, and the choice may depend on the characteristics of the data and specific requirements.


### Tree Pruning
Tree pruning in data mining refers to the process of refining or simplifying a decision tree structure to improve its performance and generalization on new, unseen data. The primary goal of tree pruning is to address the issue of overfitting, where a decision tree becomes too specific to the training data, capturing noise or outliers that may not be representative of the underlying patterns in the data.

1. **Overfitting in Decision Trees:**
   - Decision trees have a tendency to grow excessively deep, capturing details specific to the training data that may not generalize well to new, unseen data.
   - Overfitting leads to poor performance on independent test data, as the tree has essentially memorized the training set.

2. **Objectives of Tree Pruning:**
   - **Improve Generalization:** Pruning aims to create a more generalized tree that performs well on new data by removing unnecessary details captured during training.
   - **Reduce Complexity:** Pruned trees are simpler, with fewer branches and nodes, making them easier to interpret and reducing the risk of overfitting.

3. **Two Common Approaches to Tree Pruning:**
   - **Prepruning:** Involves stopping the construction of the tree at an early stage. For example, if a split would result in a small subset, further partitioning may be halted, and the node becomes a leaf.
   - **Postpruning:** Involves growing a full tree first and then removing branches. Subtrees are replaced with leaves, often labeled with the most frequent class within the subtree.

4. **Methods Used in Pruning:**
   - **Cost Complexity Pruning (Postpruning):** Used in CART, it balances the number of leaves and the error rate to determine the optimal tree size. It prunes subtrees to minimize cost complexity.
   - **Pessimistic Pruning (Postpruning):** Used in C4.5, it adjusts error rates obtained from the training set to counter optimism bias. It does not require a separate pruning set.

5. **MDL Principle for Pruning:**
   - Pruning based on the Minimum Description Length (MDL) principle involves selecting the tree that minimizes the number of bits required to encode it. This principle favors simplicity.

6. **Combined Approaches:**
   - Prepruning and postpruning can be combined for a more balanced approach, leveraging the advantages of both strategies.

7. **Challenges with Decision Trees:**
   - Even pruned trees can be large and complex, leading to repetition and replication issues. Multivariate splits or alternative knowledge representations, such as rules, may be considered to address these challenges.

8. **Considerations:**
   - The choice of pruning method may depend on the characteristics of the data, the desired level of tree complexity, and the available computing resources.

In summary, tree pruning is a crucial step in the development of decision trees, ensuring that they generalize well to new data and remain interpretable. The choice of pruning method depends on various factors, and a balanced approach often yields the best results.

---

## Association Rule Mining:
- Association rule mining is a popular and well researched method for discovering interesting relations between variables in large databases.
- It is intended to identify strong rules discovered in databases using different measures of interestingness.
- Based on the concept of strong rules, RakeshAgrawal et al. introduced association rules.

- Association rule mining finds interesting associations and relationships among large sets of data items. This rule shows how frequently a itemset occurs in a transaction. A typical example is a Market Based Analysis.

- Market Based Analysis is one of the key techniques used by large relations to show associations between items.It allows retailers to identify relationships between the items that people buy together frequently.

### Basic definitions.

- Support Count – Frequency of occurrence of a itemset.
- Frequent Itemset – An itemset whose support is greater than or equal to minsup threshold.
- Association Rule – An implication expression of the form X -> Y, where X and Y are any 2 itemsets.

### Rule Evaluation Metrics –
1. Support(s) –
  - The number of transactions that include items in the {X} and {Y} parts of the rule as a percentage of the total number of transaction.It is a measure of how frequently the collection of items occur together as a percentage of all transactions.
  - It is interpreted as fraction of transactions that contain both X and Y.
2. Confidence(c) –
  - It is the ratio of the no of transactions that includes all items in {B} as well as the no of transactions that includes all items in {A} to the no of transactions that includes all items in {A}.
  - It measures how often each item in Y appears in transactions that contains items in X also.
3. Lift(l) –
  - The lift of the rule X=>Y is the confidence of the rule divided by the expected confidence, assuming that the itemsets X and Y are independent of each other.The expected confidence is the confidence divided by the frequency of {Y}.
  - Lift value near 1 indicates X and Y almost often appear together as expected, greater than 1 means they appear together more than expected and less than 1 means they appear less than expected.Greater lift values indicate stronger association.

- The Association rule is very useful in analyzing datasets. The data is collected using bar-code scanners in supermarkets. Such databases consists of a large number of transaction records which list all items bought by a customer on a single purchase. So the manager could know if certain groups of items are consistently purchased together and use this data for adjusting store layouts, cross-selling, promotions based on statistics.

### Market basket analysis:
This process analyzes customer buying habits by finding associations between the different items that customers place in their shopping baskets. The discovery of such associations can help retailers develop marketing strategies by gaining insight into which items are frequently purchased together by customers. 

For instance, if customers are buying milk, how likely are they to also buy bread (and what kind of bread) on the same trip to the supermarket. Such information can lead to increased sales by helping retailers do selective marketing and plan their shelf space.


### Frequent Pattern Mining:
Frequent patternmining can be classified in various ways, based on the following criteria:

1. Based on the completeness of patterns to be mined:
  - We can mine the complete set of frequent itemsets, the closed frequent itemsets, and the maximal frequent itemsets, given a minimum support threshold.
  - We can also mine constrained frequent itemsets, approximate frequent itemsets,near- match frequent itemsets, top-k frequent itemsets and so on.

2. Based on the levels of abstraction involved in the rule set:
  - Some methods for association rule mining can find rules at differing levels of abstraction.
  - For example, suppose that a set of association rules mined includes the following rules where X is a variable representing a customer:
    - buys(X, "computer"))=>buys(X, "HP printer") (1)
    - buys(X, "laptop computer")) =>buys(X, "HP printer") (2)
  - In rule (1) and (2), the items bought are referenced at different levels of abstraction (e.g., "computer" is a higher-level abstraction of "laptop computer").

3. Based on the number of data dimensions involved in the rule:
  - If the items or attributes in an association rule reference only one dimension, then it is a single-dimensional association rule.
    - buys(X, "computer"))=>buys(X, "antivirus software")
  - If a rule references two or more dimensions, such as the dimensions age, income, and buys, then it is a multidimensional association rule. The following rule is an exampleof a multidimensional rule:
    - age(X, "30,31…39") ^ income(X, "42K,…48K"))=>buys(X, "high resolution TV")

4. Based on the types of values handled in the rule:
  - If a rule involves associations between the presence or absence of items, it is a Boolean association rule.
  - If a rule describes associations between quantitative items or attributes, then it is a quantitative association rule.

5. Based on the kinds of rules to be mined:
  - Frequent pattern analysis can generate various kinds of rules and other interesting relationships.
  - Association rule mining can generate a large number of rules, many of which are redundant or do not indicate a correlation relationship among itemsets.
  - The discovered associations can be further analyzed to uncover statistical correlations, leading to correlation rules.

6. Based on the kinds of patterns to be mined:
  - Many kinds of frequent patterns can be mined from different kinds of data sets.
  - Sequential pattern mining searches for frequent subsequences in a sequence data set, where a sequence records an ordering of events.
  - For example, with sequential pattern mining, we can study the order in which items are frequently purchased. For instance, customers may tend to first buy a PC, followed by a digital camera,and then a memory card.
  - Structured pattern mining searches for frequent substructures in a structured data set.
  - Single items are the simplest form of structure.
  - Each element of an itemset may contain a subsequence, a subtree, and so on.
  - Therefore, structured pattern mining can be considered as the most general form of frequent pattern mining.
